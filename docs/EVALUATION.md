# Evaluation Methodology

A robust evaluation framework is the cornerstone of this project. Its purpose is to objectively measure and compare the performance of different Retrieval-Augmented Generation (RAG) pipelines. This document details the metrics we use, the datasets we benchmark against, and the protocol for running and interpreting evaluations.

## 1. Evaluation Philosophy

Our goal is to answer two fundamental questions for each pipeline:

1.  **Retrieval Quality:** How effective is the pipeline at finding the single, correct chart or table from a large collection that can answer the user's question?
2.  **Answer Quality:** Once the correct chart is found, how accurately can the pipeline use its content to generate the correct answer?

To this end, our evaluation is divided into two main categories of metrics: **Retrieval Metrics** and **Question-Answering (QA) Metrics**. We also measure component-level metrics (like OCR quality) to diagnose and debug the system.

## 2. Core Evaluation Metrics

All metrics are calculated by the `src/eval/metrics.py` module and orchestrated by `eval.py`.

### 2.1. Retrieval Performance Metrics

These metrics assess the performance of the retrieval step, which is the foundation of the RAG system.

| Metric                 | Description                                                                                             | Interpretation                                                                    |
| ---------------------- | ------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------- |
| **Recall@K**           | What percentage of the time is the correct source document found within the top *K* retrieved results?    | A high Recall@5 means the user will almost always find the right chart in the top 5 suggestions. |
| **MRR (Mean Reciprocal Rank)** | Measures the average inverse rank of the *first* correct document. A score of 1 is perfect (always ranked first). | A higher MRR indicates the system is not only finding the correct document but also ranking it highly. |

### 2.2. Generation (Question-Answering) Metrics

These metrics evaluate the final answer generated by the Large Language Model (LLM) against a ground-truth answer.

| Metric                 | Description                                                                                             | Interpretation                                                                    |
| ---------------------- | ------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------- |
| **Exact Match (EM)**   | The percentage of answers that are an exact string match with the ground truth (after normalization).     | Very strict. A good score indicates high precision but can be sensitive to phrasing differences. |
| **F1 Score**           | The harmonic mean of precision and recall of words in the answer. Robust to minor wording differences.    | The primary metric for overall textual accuracy. It rewards answers that share the most words with the ground truth. |
| **ROUGE-L**            | Measures the longest common subsequence of words between the prediction and the ground truth.             | Similar to F1, useful for evaluating longer, more descriptive answers.              |
| **Numeric Accuracy**   | For answers containing numbers, is the predicted number correct within a given relative tolerance?        | **Crucial for this project.** This tells us if the system can accurately read data points from charts. |

### 2.3. Component-Level Metrics

These metrics help us isolate issues within the pipeline.

| Metric                             | Component | Description                                                        | Interpretation                                                |
| ---------------------------------- | --------- | ------------------------------------------------------------------ | ------------------------------------------------------------- |
| **CER (Character Error Rate)**     | OCR       | The percentage of characters incorrectly transcribed by the OCR engine. | A low CER is essential for the `ocr-text-vec` pipeline. Lower is better. |
| **WER (Word Error Rate)**          | OCR       | The percentage of words incorrectly transcribed by the OCR engine.    | A high WER can explain why a pipeline is failing to answer correctly. Lower is better. |

## 3. Datasets for Benchmarking

We use a combination of standard academic datasets and internally generated synthetic data to ensure our evaluations are comprehensive.

| Dataset Name      | Description                                                                                               | Usage                                                                                        |
| ----------------- | --------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------- |
| **Synthetic Data** (`data/synthetic`) | Internally generated charts with known ground-truth data tables and QA pairs. | Excellent for controlled experiments and testing the system's core capabilities in a clean environment. |
| **ChartQA**       | A large-scale dataset with complex, human-written questions about a diverse range of charts.            | The primary benchmark for real-world performance, testing complex reasoning and visual understanding. |
| **PlotQA**        | Similar to ChartQA, focuses on questions about plots and graphs.                                          | Provides another source of complex, real-world evaluation data.                              |
| **PubTabNet**     | A massive dataset of tables from scientific publications, used for table structure recognition.            | Primarily used for evaluating the performance of the `derender-table-vec` pipeline components.   |

All datasets can be downloaded using the `scripts/download_datasets.py` script.

## 4. Evaluation Protocol

Following a standardized protocol ensures that our results are reproducible and comparable across different experiments.

### Step 1: Running the Evaluation Script
The primary tool for evaluation is `eval.py`. This script orchestrates the entire process:
1.  Loads a specified RAG pipeline and its corresponding index.
2.  Loads a labeled evaluation dataset (questions, answers, and relevant document IDs).
3.  Iterates through every question in the dataset.
4.  For each question, it runs the pipeline to get a predicted answer and retrieved documents.
5.  It compares the prediction against the ground truth and calculates all relevant metrics.
6.  It aggregates the results and saves them.

**Example Command:**
```bash
python eval.py \
    --pipeline-name "ocr-text-vec" \
    --index-dir outputs/indexes/chartqa_text_index \
    --eval-dataset data/raw/chartqa/test.jsonl \
    --output-dir outputs/eval_results/run_001 \
    --experiment-name "baseline_tesseract_on_chartqa"
```

### Step 2: Analyzing the Output
Each evaluation run produces several key artifacts in the specified output directory:
*   `evaluation_results.csv`: A detailed, row-by-row breakdown of every question, its prediction, ground truth, and all calculated scores. This is invaluable for error analysis.
*   `summary.json`: A summary file containing the average scores for all metrics across the entire dataset.
*   **W&B Logs:** If configured, all summary metrics and the detailed results table are automatically logged to Weights & Biases for easy visualization and comparison across runs.

### Step 3: Comparing Pipelines and Error Analysis
The ultimate goal is to compare different approaches.
*   **Quantitative Comparison:** Use the `summary.json` files or the W&B dashboard to compare the average metrics (e.g., F1 Score, Numeric Accuracy, Recall@5) between different pipeline runs.
*   **Statistical Significance:** For rigorous analysis, the `MetricsCalculator` includes functions for paired t-tests or Wilcoxon tests to determine if the performance difference between two pipelines is statistically significant.
*   **Qualitative Error Analysis:** Dive deep into the `evaluation_results.csv` file. By filtering for rows with low F1 scores or incorrect exact matches, you can identify patterns in failures. For instance:
    *   *Are failures concentrated in a specific chart type?*
    *   *Is the OCR engine consistently misreading a specific character (e.g., '5' for 'S')?*
    *   *Is the LLM misinterpreting the retrieved context?*

This combination of quantitative metrics and qualitative analysis allows us to build a comprehensive understanding of each pipeline's strengths and weaknesses, leading to targeted improvements.