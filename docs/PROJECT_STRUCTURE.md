# RAG Charts Project - Complete File Structure

This document outlines all Python files needed for the complete implementation of the RAG for Charts & Tables project.

## âœ… Files Already Created

### Configuration & Utilities
1. `requirements.txt` - All dependencies
2. `config/config.yaml` - Main configuration file
3. `src/utils/config.py` - Configuration management
4. `src/utils/logger.py` - Logging setup using loguru
5. `src/utils/tracking.py` - Experiment tracking (W&B/MLflow)

### Core Modules
6. `src/ingestion/preprocessor.py` - Image preprocessing and figure detection
7. `src/ocr/engines.py` - OCR engines (Tesseract, TrOCR, Donut)
8. `src/derender/extractors.py` - Chart derendering and table extraction
9. `src/encoders/embedders.py` - Image, text, and table embedders
10. `src/index/vector_store.py` - FAISS and Milvus vector stores
11. `src/rag_pipelines/orchestrator.py` - RAG pipeline implementations

## ğŸ“ Additional Files Needed

### Main Scripts
12. **`run_ingest.py`** - Main ingestion script
```python
# Orchestrates: image loading, preprocessing, figure detection, saving metadata
# CLI interface for batch processing documents
```

13. **`run_ocr.py`** - OCR processing script
```python
# Runs OCR engines on images, saves results with confidence scores
# Supports parallel processing, error handling, and logging
```

14. **`run_derender.py`** - Chart derendering script
```python
# Runs chart-to-table conversion and table extraction
# Saves structured CSV/JSON outputs
```

15. **`index_build.py`** - Index building script
```python
# Builds FAISS/Milvus indexes from embeddings
# Handles image, text, and table embeddings separately
```

16. **`query_rag.py`** - Query interface script
```python
# CLI/API for querying RAG pipelines
# Supports all pipeline variants via flags
```

### Evaluation Module
17. **`src/eval/metrics.py`** - Evaluation metrics
```python
# Functions for: Recall@k, MRR, Exact Match, F1, ROUGE-L, CER, WER
# Statistical significance testing
```

18. **`src/eval/evaluator.py`** - Main evaluator class
```python
# Orchestrates evaluation across pipelines and datasets
# Generates comparison tables and visualizations
```

19. **`eval.py`** - Evaluation runner script
```python
# Runs full evaluation suite on test sets
# Generates reports and saves results
```

20. **`run_ablation.py`** - Ablation study runner
```python
# Runs matrix of experiments (engines Ã— encoders Ã— settings)
# Logs all results to experiment tracking
```

### Data Generation
21. **`data/synthetic/generate_charts.py`** - Synthetic chart generator
```python
# Uses matplotlib/plotly to generate synthetic charts
# Creates controlled variants (fonts, noise, rotations, etc.)
# Saves ground-truth CSV data
```

22. **`src/ingestion/dataset_loader.py`** - Dataset loaders
```python
# Loaders for ChartQA, PlotQA, PubTabNet, DocVQA
# Standardized interface for all datasets
```

### LLM Integration
23. **`src/rag_pipelines/llm_client.py`** - LLM client abstraction
```python
# Unified interface for OpenAI, Anthropic, local models
# Handles rate limiting, retries, token counting
```

24. **`src/rag_pipelines/prompts.py`** - Prompt templates
```python
# Templates for different pipeline types
# Includes few-shot examples and formatting
```

### Analysis & Reporting
25. **`notebooks/01_exploratory_analysis.ipynb`** - EDA notebook
```python
# Dataset statistics, visualization examples
# OCR quality analysis, embedding space visualization
```

26. **`notebooks/02_results_analysis.ipynb`** - Results analysis
```python
# Comparative analysis of pipelines
# Statistical testing, error analysis
# Publication-ready plots
```

27. **`src/eval/report_generator.py`** - Report generator
```python
# Generates final report (markdown/PDF)
# Creates tables, plots, failure analysis
```

### Utilities
28. **`src/utils/data_utils.py`** - Data utilities
```python
# Common data processing functions
# File I/O, format conversions, batch processing
```

29. **`src/utils/visualization.py`** - Visualization utilities
```python
# Plotting functions for embeddings, attention, results
# Chart overlay visualization with bboxes
```

30. **`src/utils/text_processing.py`** - Text processing utilities
```python
# Text normalization, numeric extraction
# Table flattening helpers
```

### Testing
31. **`tests/test_ocr.py`** - OCR engine tests
32. **`tests/test_embedders.py`** - Embedder tests
33. **`tests/test_vector_store.py`** - Vector store tests
34. **`tests/test_pipelines.py`** - Pipeline tests
35. **`tests/test_evaluation.py`** - Evaluation tests

### Docker & Deployment
36. **`Dockerfile`** - Main Dockerfile
```dockerfile
# Multi-stage build for production
# Includes all dependencies and models
```

37. **`docker-compose.yml`** - Docker compose for services
```yaml
# Services: app, milvus, postgres (for metadata)
```

38. **`scripts/download_models.py`** - Model downloader
```python
# Downloads all required HuggingFace models
# Sets up cache directories
```

### API (Optional)
39. **`api/app.py`** - FastAPI application
```python
# REST API for RAG queries
# Endpoints for each pipeline type
```

40. **`api/schemas.py`** - API schemas
```python
# Pydantic models for requests/responses
```

### Documentation
41. **`README.md`** - Main README
42. **`docs/INSTALLATION.md`** - Installation guide
43. **`docs/USAGE.md`** - Usage guide
44. **`docs/EVALUATION.md`** - Evaluation methodology
45. **`docs/API.md`** - API documentation

### Configuration
46. **`.env.example`** - Environment variables template
47. **`.gitignore`** - Git ignore file
48. **`setup.py`** - Package setup script
49. **`pyproject.toml`** - Modern Python project config

### Experiment Configs
50. **`experiments/exp_001_baseline.yaml`** - Baseline experiment
51. **`experiments/exp_002_ocr_comparison.yaml`** - OCR comparison
52. **`experiments/exp_003_encoder_ablation.yaml`** - Encoder ablation

## ğŸ—ï¸ Directory Structure

```
rag_charts_project/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.yaml
â”‚   â””â”€â”€ experiment_configs/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â”œâ”€â”€ labels/
â”‚   â””â”€â”€ synthetic/
â”‚       â””â”€â”€ generate_charts.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ preprocessor.py âœ…
â”‚   â”‚   â””â”€â”€ dataset_loader.py
â”‚   â”œâ”€â”€ ocr/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ engines.py âœ…
â”‚   â”œâ”€â”€ derender/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ extractors.py âœ…
â”‚   â”œâ”€â”€ encoders/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ embedders.py âœ…
â”‚   â”œâ”€â”€ index/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ vector_store.py âœ…
â”‚   â”œâ”€â”€ rag_pipelines/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ orchestrator.py âœ…
â”‚   â”‚   â”œâ”€â”€ llm_client.py
â”‚   â”‚   â””â”€â”€ prompts.py
â”‚   â”œâ”€â”€ eval/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ metrics.py
â”‚   â”‚   â”œâ”€â”€ evaluator.py
â”‚   â”‚   â””â”€â”€ report_generator.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py âœ…
â”‚       â”œâ”€â”€ logger.py âœ…
â”‚       â”œâ”€â”€ tracking.py âœ…
â”‚       â”œâ”€â”€ data_utils.py
â”‚       â”œâ”€â”€ visualization.py
â”‚       â””â”€â”€ text_processing.py
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_exploratory_analysis.ipynb
â”‚   â””â”€â”€ 02_results_analysis.ipynb
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_ocr.py
â”‚   â”œâ”€â”€ test_embedders.py
â”‚   â”œâ”€â”€ test_vector_store.py
â”‚   â”œâ”€â”€ test_pipelines.py
â”‚   â””â”€â”€ test_evaluation.py
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ schemas.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download_models.py
â”‚   â””â”€â”€ setup_env.sh
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ exp_001_baseline.yaml
â”‚   â”œâ”€â”€ exp_002_ocr_comparison.yaml
â”‚   â””â”€â”€ exp_003_encoder_ablation.yaml
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ INSTALLATION.md
â”‚   â”œâ”€â”€ USAGE.md
â”‚   â”œâ”€â”€ EVALUATION.md
â”‚   â””â”€â”€ API.md
â”œâ”€â”€ run_ingest.py âœ…
â”œâ”€â”€ run_ocr.py âœ…
â”œâ”€â”€ run_derender.py âœ…
â”œâ”€â”€ index_build.py âœ…
â”œâ”€â”€ query_rag.py âœ…
â”œâ”€â”€ eval.py âœ…
â”œâ”€â”€ run_ablation.py âœ…
â”œâ”€â”€ requirements.txt âœ…
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ setup.py
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ PROJECT_STRUCTURE.md âœ…
```

## ğŸš€ Quick Start Commands

```bash
# 1. Setup environment
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# 2. Download models
python scripts/download_models.py

# 3. Process data
python run_ingest.py --input data/raw --output data/processed

# 4. Run OCR
python run_ocr.py --input data/processed --engine tesseract,trocr

# 5. Build indexes
python index_build.py --config config/config.yaml

# 6. Query
python query_rag.py --query "What was the GDP in 2020?" --pipeline ocr-text-vec

# 7. Evaluate
python eval.py --config config/config.yaml --output results/

# 8. Run ablations
python run_ablation.py --config experiments/exp_002_ocr_comparison.yaml
```

## ğŸ“Š Key Implementation Notes

### Priority Order for Remaining Files:
1. **High Priority** (Core functionality):
   - run_ingest.py, run_ocr.py, index_build.py, query_rag.py
   - src/eval/metrics.py, src/eval/evaluator.py
   - src/rag_pipelines/llm_client.py

2. **Medium Priority** (Analysis & Testing):
   - eval.py, run_ablation.py
   - src/utils/data_utils.py, text_processing.py
   - tests/* files

3. **Low Priority** (Nice to have):
   - API files, notebooks, docs
   - Deployment files (Docker, etc.)

### Design Principles Applied:
- âœ… Modular architecture with clear separation of concerns
- âœ… Consistent interfaces across all components
- âœ… Comprehensive error handling and logging
- âœ… Configuration-driven design (no hardcoded values)
- âœ… Type hints throughout
- âœ… Experiment tracking integration
- âœ… Statistical rigor in evaluation

### Next Steps:
1. Implement remaining high-priority files
2. Create minimal test suite
3. Generate synthetic dataset for testing
4. Run end-to-end smoke test
5. Begin baseline experiments